# machine-learning-fundamentals

Machine Learning Fundamentals

- **Gradient descent**

  - is a method to optimize your linear models.

- **Multiple Linear Regression**

  - is a technique for when you are comparing more than two variables.

- **Polynomial Regression**

  - is for relationships between variables that aren't linear.

- **Regularization**

  - is a technique to assure that your models will not only fit to the data available, but also extend to new situations.

- **Regression Problem**
  - predicting quantitative values. Predicting quantitative values is often just considered a Regression problem.

## Book

[Deep Learning Book, Ian Goodfellow](https://www.deeplearningbook.org/)

## ML FUNDAMENTALS

1. INTRO TO MACHINE LEARNING

   - 1. What Is Machine Learning?
   - 2. Projects

2. Intro

   - 1. Python Installation
   - 2. [For Windows] Configuring Git Bash to Run Python
   - 3. What is Anaconda?
   - 11. Installing Anaconda
   - 12. Managing packages
   - 13. On Python versions at Udacity
   - 14. Running a Python Script
   - 15. Programming Environment Setup
   - 16. What are Jupyter notebooks?
   - 17. Installing Jupyter Notebook
   - 18. Launching the notebook server
   - 19. Notebook interface
   - 20. Markdown cells
   - 21. Code cells
   - 22. Keyboard shortcuts
   - 23. Magic keywords
   - 24. Converting notebooks
   - 25. Creating a slideshow
   - 26. Outro

## SUPERVISED LEARNING

### LESSON 1: Machine Learning Bird's Eye View

- 1.  Introduction
- 2.  History - A Statistician's Perspective
- 3.  History - A Computer Scientist's Perspective
- 4.  Types of Machine Learning - Supervised
- 5.  Types of Machine Learning - Unsupervised & Reinforcement
- 6.  Deep Learning
- 7.  Scikit Learn
- 8.  Ethics in Machine Learning
- 9.  What's Ahead
- 10. Text: Recap

### LESSON 2: Linear Regression

- 1.  Intro
- 2.  Quiz: Housing Prices
- 3.  Solution: Housing Prices
- 4.  Fitting a Line Through Data
- 5.  Moving a Line
- 6.  Absolute Trick
- 7.  Square Trick
- 8.  Quiz: Absolute and Square Trick
- 9.  Gradient Descent
- 10. Mean Absolute Error
- 11. Mean Squared Error
- 12. Quiz: Mean Absolute & Squared Errors
- 13. Minimizing Error Functions
- 14. Mean vs Total Error
- 15. Mini-batch Gradient Descent
- 16. Quiz: Mini-Batch Gradient Descent
- 17. Absolute Error vs Squared Error
- 18. Linear Regression in Scikit learn
- 19. Higher Dimensions
- 20. Multiple Linear Regression
- 21. Closed Form Solution
- 22. (Optional) Closed form Solution Math
- 23. Linear Regression Warnings
- 24. Polynomial Regression
- 25. Quiz: Polynomial Regression
- 26. Regularization
- 27. Quiz: Regularization
- 28. Feature Scaling
- 29. Outro

### LESSON 3: Perceptron Algorithm

- 1. Intro
- 2.  Classification Problems 1
- 3.  Classification Problems 2
- 4.  Linear Boundaries
- 5.  Higher Dimensions
- 6.  Perceptrons
- 7.  Perceptrons as Logical Operators
- 8.  Perceptron Trick
- 9.  Perceptron Algorithm
- 10. Outro

### LESSON 4: Decision Trees

- 1.  Instructor
- 2.  Introduction
- 3.  Classification Problems 1
- 4.  Classification Problems 2
- 5.  Linear Boundaries
- 6.  Higher Dimensions
- 7.  Perceptrons
- 8.  Why "Neural Networks"?
- 9.  Perceptrons as Logical Operators
- 10. Perceptron Trick
- 11. Perceptron Algorithm
- 12. Non-Linear Regions
- 13. Error Functions
- 14. Log-loss Error Function
- 15. Discrete vs Continuous
- 16. Softmax
- 17. One-Hot Encoding
- 18. Maximum Likelihood
- 19. Maximizing Probabilities
- 20. Cross-Entropy 1
- 21. Cross-Entropy 2
- 22. Multi-Class Cross Entropy
- 23. Logistic Regression
- 24. Gradient Descent
- 25. Logistic Regression Algorithm
- 26. Pre-Lab: Gradient Descent
- 27. Notebook: Gradient Descent
- 28. Perceptron vs Gradient Descent
- 29. Continuous Perceptrons
- 30. Non-linear Data
- 31. Non-Linear Models
- 32. Neural Network Architecture
- 33. Feedforward
- 34. Backpropagation
- 35. Pre-Lab: Analyzing Student Data
- 36. Notebook: Analyzing Student Data
- 37. Outro

### LESSON 5: Naive Bayes

- 1.  Mean Squared Error Function
- 2.  Gradient Descent
- 3.  Gradient Descent: The Math
- 4.  Gradient Descent: The Code
- 5.  Implementing Gradient Descent
- 6.  Multilayer Perceptrons
- 7.  Backpropagation
- 8.  Implementing Backpropagation
- 9.  Further Reading

### LESSON 6: Support Vector Machines

- 1. Instructor
- 2.  Training Optimization
- 3.  Testing
- 4.  Overfitting and Underfitting
- 5.  Early Stopping
- 6.  Regularization
- 7.  Regularization 2
- 8.  Dropout
- 9.  Local Minima
- 10. Random Restart
- 11. Vanishing Gradient
- 12. Other Activation Functions
- 13. Batch vs Stochastic Gradient Descent
- 14. Learning Rate Decay
- 15. Momentum
- 16. Error Functions Around the World

### LESSON 7: Ensemble Methods

- 1. Intro
- 2. Ensembles
- 3. Random Forests
- 4. Bagging
- 5. AdaBoost
- 6. Weighting the Data
- 7. Weighting the Models 1
- 8. Weighting the Models 2

### LESSON 8: Model Evaluation Metrics

- 1. Intro
- 2. Outline
- 3. Testing your models
- 4. Confusion Matrix
- 5. Confusion Matrix 2
- 6. Accuracy
- 7. Accuracy 2
- 8. When accuracy won't work

### LESSON 9: Training and Tuning

- 1. Types of Errors
- 2. Model Complexity Graph
- 3. Cross Validation
- 4. K-Fold Cross Validation
- 5. Learning Curves
- 6. Detecting Overfitting and Underfitting
- 7. Solution: Detecting Overfitting and Underfitting
- 8. Grid Search

### PROJECT: Finding Donors for CharityML

- 1. Overview
- 2. Software Requirements
- 3. Starting the Project
- 4. Submitting the project
- 5. Project Workspace
- 6. [Optional] Kaggle Competition
- 7. [Optional] Kaggle Competition
- 8. Project: Finding Donors for CharityML
